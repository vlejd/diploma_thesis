{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "import logging\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "import gensim\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.linear_model import *\n",
    "import scipy.sparse as sps\n",
    "from sklearn.model_selection import KFold\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from sklearn import linear_model\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "fname = \"data/train.csv\"\n",
    "data = pd.read_csv(fname, header=0)\n",
    "head = data.head().copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "work_data = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:600000\n",
      "INFO:root:700000\n",
      "INFO:root:800000\n",
      "INFO:root:900000\n",
      "INFO:root:1000000\n",
      "INFO:root:1100000\n",
      "INFO:root:1200000\n",
      "INFO:root:1300000\n"
     ]
    }
   ],
   "source": [
    "work_data.question1 = data.question1.apply(normalize)\n",
    "work_data.question2 = data.question2.apply(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.corpora.dictionary:adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO:gensim.corpora.dictionary:adding document #10000 to Dictionary(11705 unique tokens: ['installing', 'which', '5ft', 'mla', 'combustible']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #20000 to Dictionary(16732 unique tokens: ['suicides', 'nightclubs', 'installing', 'euntrepreneur', 'which']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #30000 to Dictionary(20448 unique tokens: ['suicides', 'nightclubs', 'installing', 'euntrepreneur', 'resetting']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #40000 to Dictionary(23451 unique tokens: ['racing', 'library', 'bombay', 'realistic', 'mla']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #50000 to Dictionary(26283 unique tokens: ['racing', 'library', 'bombay', 'ziffi', 'icmr']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #60000 to Dictionary(28645 unique tokens: ['racing', 'library', 'bombay', 'ziffi', 'icmr']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #70000 to Dictionary(30775 unique tokens: ['racing', 'library', 'bombay', 'ziffi', 'subscribed']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #80000 to Dictionary(32603 unique tokens: ['racing', 'agoraphobia', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #90000 to Dictionary(34376 unique tokens: ['racing', 'tetracycline', 'agoraphobia', 'library', 'bombay']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #100000 to Dictionary(36084 unique tokens: ['zazen', 'racing', 'tetracycline', 'agoraphobia', 'library']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #110000 to Dictionary(37671 unique tokens: ['zazen', 'racing', 'tetracycline', 'agoraphobia', '63kgs']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #120000 to Dictionary(39160 unique tokens: ['zazen', 'racing', 'tetracycline', 'agoraphobia', '63kgs']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #130000 to Dictionary(40625 unique tokens: ['zazen', 'racing', 'tetracycline', 'agoraphobia', '63kgs']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #140000 to Dictionary(41980 unique tokens: ['zazen', 'racing', 'tetracycline', 'agoraphobia', '63kgs']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #150000 to Dictionary(43345 unique tokens: ['zazen', 'racing', 'tetracycline', 'agoraphobia', '63kgs']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #160000 to Dictionary(44644 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #170000 to Dictionary(45878 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #180000 to Dictionary(47101 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #190000 to Dictionary(48225 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #200000 to Dictionary(49362 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #210000 to Dictionary(50422 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #220000 to Dictionary(51426 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #230000 to Dictionary(52460 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #240000 to Dictionary(53431 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #250000 to Dictionary(54388 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #260000 to Dictionary(55373 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #270000 to Dictionary(56284 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #280000 to Dictionary(57176 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #290000 to Dictionary(58076 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #300000 to Dictionary(58956 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #310000 to Dictionary(59819 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #320000 to Dictionary(60655 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #330000 to Dictionary(61503 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #340000 to Dictionary(62272 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #350000 to Dictionary(62991 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #360000 to Dictionary(63709 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #370000 to Dictionary(64464 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #380000 to Dictionary(65212 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #390000 to Dictionary(65973 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #400000 to Dictionary(66669 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #410000 to Dictionary(67301 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #420000 to Dictionary(67828 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #430000 to Dictionary(68398 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #440000 to Dictionary(68876 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #450000 to Dictionary(69364 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #460000 to Dictionary(69854 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #470000 to Dictionary(70361 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #480000 to Dictionary(70884 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #490000 to Dictionary(71372 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #500000 to Dictionary(71865 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #510000 to Dictionary(72336 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #520000 to Dictionary(72779 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #530000 to Dictionary(73213 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #540000 to Dictionary(73718 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #550000 to Dictionary(74193 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #560000 to Dictionary(74672 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #570000 to Dictionary(75114 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #580000 to Dictionary(75558 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #590000 to Dictionary(75994 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #600000 to Dictionary(76372 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #610000 to Dictionary(76793 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #620000 to Dictionary(77205 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #630000 to Dictionary(77679 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #640000 to Dictionary(78090 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #650000 to Dictionary(78539 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #660000 to Dictionary(78989 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #670000 to Dictionary(79406 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #680000 to Dictionary(79835 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #690000 to Dictionary(80225 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #700000 to Dictionary(80628 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #710000 to Dictionary(81024 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #720000 to Dictionary(81427 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #730000 to Dictionary(81798 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #740000 to Dictionary(82179 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #750000 to Dictionary(82571 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #760000 to Dictionary(82952 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #770000 to Dictionary(83367 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #780000 to Dictionary(83741 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #790000 to Dictionary(84143 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:adding document #800000 to Dictionary(84512 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...)\n",
      "INFO:gensim.corpora.dictionary:built Dictionary(84857 unique tokens: ['icmr', 'tetracycline', 'library', 'bombay', 'hummer']...) from 808580 documents (total 10234972 corpus positions)\n",
      "INFO:gensim.corpora.dictionary:discarding 45208 tokens: [('is', 257252), ('what', 316059), ('the', 304592), ('in', 179436), ('to', 180810), ('?', 807577), ('of', 144031), ('i', 176129), ('can', 115668), ('a', 178049)]...\n",
      "INFO:gensim.corpora.dictionary:keeping 39649 tokens which were in no less than 3 and no more than 80858 (=10.0%) documents\n",
      "INFO:gensim.corpora.dictionary:resulting dictionary: Dictionary(39649 unique tokens: ['nucleolus', 'icmr', 'tetracycline', 'agoraphobia', 'schick']...)\n"
     ]
    }
   ],
   "source": [
    "pom = pd.concat([work_data.question1, work_data.question2])\n",
    "dic = Dictionary(pom)\n",
    "dic.filter_extremes(no_below=3, no_above=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "work_data.question1 = work_data.question1.apply(lambda x: dic.doc2bow(x))\n",
    "work_data.question2 = work_data.question2.apply(lambda x: dic.doc2bow(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'work_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-c38eaa698f52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwork_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/train_vectorized.pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/dictionary.dic\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'work_data' is not defined"
     ]
    }
   ],
   "source": [
    "work_data.to_pickle(\"data/train_vectorized.pickle\")\n",
    "dic.save(\"data/dictionary.dic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sklearn.model_selection.GridSearchCV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.utils:loading Dictionary object from data/dictionary.dic\n",
      "INFO:gensim.utils:loaded data/dictionary.dic\n"
     ]
    }
   ],
   "source": [
    "work_data = pd.read_pickle(\"data/train_vectorized.pickle\")\n",
    "dic = gensim.corpora.Dictionary.load(\"data/dictionary.dic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models.tfidfmodel:collecting document frequencies\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #0\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #10000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #20000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #30000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #40000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #50000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #60000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #70000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #80000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #90000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #100000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #110000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #120000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #130000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #140000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #150000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #160000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #170000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #180000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #190000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #200000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #210000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #220000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #230000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #240000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #250000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #260000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #270000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #280000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #290000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #300000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #310000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #320000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #330000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #340000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #350000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #360000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #370000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #380000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #390000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #400000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #410000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #420000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #430000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #440000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #450000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #460000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #470000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #480000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #490000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #500000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #510000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #520000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #530000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #540000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #550000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #560000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #570000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #580000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #590000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #600000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #610000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #620000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #630000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #640000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #650000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #660000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #670000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #680000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #690000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #700000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #710000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #720000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #730000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #740000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #750000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #760000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #770000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #780000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #790000\n",
      "INFO:gensim.models.tfidfmodel:PROGRESS: processing document #800000\n",
      "INFO:gensim.models.tfidfmodel:calculating IDF weights for 808580 documents and 39648 features (6062806 matrix non-zeros)\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfModel(pd.concat([work_data.question1, work_data.question2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "work_data.question1 = work_data.question1.apply(lambda x: tfidf[x])\n",
    "work_data.question2 = work_data.question2.apply(lambda x: tfidf[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4307, 0.3104637458592939),\n",
       " (31103, 0.3249384776224683),\n",
       " (36323, 0.3953716277550753),\n",
       " (36609, 0.19369471786822115),\n",
       " (36728, 0.29112713390459066),\n",
       " (37302, 0.7207189968414592)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "work_data.question2.head()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q1 = gensim.matutils.corpus2csc(work_data.question1).transpose()\n",
    "q2 = gensim.matutils.corpus2csc(work_data.question2).transpose()\n",
    "q1 = sps.csr_matrix((q1.data, q1.indices, q1.indptr), shape=(work_data.shape[0], max(dic.dfs.keys())+1))\n",
    "q2 = sps.csr_matrix((q2.data, q2.indices, q2.indptr), shape=(work_data.shape[0], max(dic.dfs.keys())+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404290, 39649)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = q1.multiply(q2)\n",
    "X = X.sqrt()\n",
    "normalizer = (((q1+q2)>0).sum(axis=1)+1)*0 + 1\n",
    "row_indices, col_indices = X.nonzero()\n",
    "X.data /= normalizer[row_indices].A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y = work_data.is_duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clsClass = MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.52417773\n",
      "Iteration 2, loss = 0.44734636\n",
      "Iteration 3, loss = 0.41202244\n",
      "Iteration 4, loss = 0.38647826\n",
      "Iteration 5, loss = 0.36548581\n",
      "Iteration 6, loss = 0.34630563\n",
      "Iteration 7, loss = 0.32869272\n",
      "Iteration 8, loss = 0.31193208\n",
      "Iteration 9, loss = 0.29580378\n",
      "Iteration 10, loss = 0.28021747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.526889424452\n",
      "Iteration 1, loss = 0.52301382\n",
      "Iteration 2, loss = 0.44414232\n",
      "Iteration 3, loss = 0.40734260\n",
      "Iteration 4, loss = 0.37996068\n",
      "Iteration 5, loss = 0.35684377\n",
      "Iteration 6, loss = 0.33539139\n",
      "Iteration 7, loss = 0.31566567\n",
      "Iteration 8, loss = 0.29699220\n",
      "Iteration 9, loss = 0.27882285\n",
      "Iteration 10, loss = 0.26269357\n",
      "0.532677161973\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=2)\n",
    "#clf = LogisticRegression(class_weight=None, max_iter=100, verbose=100, warm_start=False, n_jobs=4)\n",
    "clf = clsClass(verbose=True, max_iter=10)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    clf.fit(X[train_index],Y[train_index])\n",
    "    probs = clf.predict_proba(X[test_index])[:,1]\n",
    "    print(loss(Y[test_index], probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-7983a253843c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"data/test.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    980\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'skipfooter not supported for iteration'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 982\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'as_recarray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1717\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1719\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1720\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1721\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read (pandas/_libs/parsers.c:10862)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory (pandas/_libs/parsers.c:11138)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows (pandas/_libs/parsers.c:12175)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data (pandas/_libs/parsers.c:14136)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens (pandas/_libs/parsers.c:14858)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype (pandas/_libs/parsers.c:15629)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_integer_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mis_integer_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    738\u001b[0m     \"\"\"\n\u001b[1;32m    739\u001b[0m     \u001b[0mCheck\u001b[0m \u001b[0mwhether\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mprovided\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mof\u001b[0m \u001b[0man\u001b[0m \u001b[0minteger\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fname = \"data/test.csv\"\n",
    "test = pd.read_csv(fname, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:900000\n",
      "INFO:root:1000000\n",
      "INFO:root:1100000\n",
      "INFO:root:1200000\n",
      "INFO:root:1300000\n",
      "INFO:root:1400000\n",
      "INFO:root:1500000\n",
      "INFO:root:1600000\n",
      "INFO:root:1700000\n",
      "INFO:root:1800000\n",
      "INFO:root:1900000\n",
      "INFO:root:2000000\n",
      "INFO:root:2100000\n",
      "INFO:root:2200000\n",
      "INFO:root:2300000\n",
      "INFO:root:2400000\n",
      "INFO:root:2500000\n",
      "INFO:root:2600000\n",
      "INFO:root:2700000\n",
      "INFO:root:2800000\n",
      "INFO:root:2900000\n",
      "INFO:root:3000000\n",
      "INFO:root:3100000\n",
      "INFO:root:3200000\n",
      "INFO:root:3300000\n",
      "INFO:root:3400000\n",
      "INFO:root:3500000\n",
      "INFO:root:3600000\n",
      "INFO:root:3700000\n",
      "INFO:root:3800000\n",
      "INFO:root:3900000\n",
      "INFO:root:4000000\n",
      "INFO:root:4100000\n",
      "INFO:root:4200000\n",
      "INFO:root:4300000\n",
      "INFO:root:4400000\n",
      "INFO:root:4500000\n",
      "INFO:root:4600000\n",
      "INFO:root:4700000\n",
      "INFO:root:4800000\n",
      "INFO:root:4900000\n",
      "INFO:root:5000000\n",
      "INFO:root:5100000\n",
      "INFO:root:5200000\n",
      "INFO:root:5300000\n",
      "INFO:root:5400000\n",
      "INFO:root:5500000\n"
     ]
    }
   ],
   "source": [
    "work_test = test.copy()\n",
    "work_test.question1 = test.question1.apply(normalize)\n",
    "work_test.question2 = test.question2.apply(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "work_test.to_pickle(\"data/cleaned.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(path, compression)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtry_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mtry_read\u001b[0;34m(path, encoding)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mread_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_wrapper\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mread_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_new_Index\u001b[0;34m(cls, d)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_new_Index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m     \"\"\" This is called upon unpickling, rather than the default which doesn't\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_wrapper\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mread_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_new_Index\u001b[0;34m(cls, d)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_new_Index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m     \"\"\" This is called upon unpickling, rather than the default which doesn't\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-a25187149183>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwork_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/cleaned.pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(path, compression)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mPY3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtry_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mtry_read\u001b[0;34m(path, encoding)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;31m# GH 6899\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mread_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;31m# reg/patched pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_wrapper\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m     66\u001b[0m                             is_text=False)\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_f\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "work_test = pd.read_pickle(\"data/cleaned.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404290, 39649)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorized_test = work_test\n",
    "vectorized_test.question1 = work_test.question1.apply(lambda x: dic.doc2bow(x))\n",
    "vectorized_test.question2 = work_test.question2.apply(lambda x: dic.doc2bow(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorized_test.to_pickle(\"data/vectorized_test.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorized_test = pd.read_pickle(\"data/vectorized_test.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorized_test.question1 = vectorized_test.question1.apply(lambda x: tfidf[x])\n",
    "vectorized_test.question2 = vectorized_test.question2.apply(lambda x: tfidf[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorized_test.to_pickle(\"data/vectorized_tfidf_test.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorized_test = pd.read_pickle(\"data/vectorized_tfidf_test.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Q1 = gensim.matutils.corpus2csc(vectorized_test.question1).transpose()\n",
    "Q2 = gensim.matutils.corpus2csc(vectorized_test.question2).transpose()\n",
    "Q1 = sps.csr_matrix((Q1.data, Q1.indices, Q1.indptr), shape=(vectorized_test.shape[0], max(dic.dfs.keys())+1))\n",
    "Q2 = sps.csr_matrix((Q2.data, Q2.indices, Q2.indptr), shape=(vectorized_test.shape[0], max(dic.dfs.keys())+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_X = Q1.multiply(Q2)\n",
    "test_X[test_X>1] = 1\n",
    "\n",
    "test_X = Q1.multiply(Q2)\n",
    "test_X[test_X>0] = 1\n",
    "test_X = test_X.sqrt()\n",
    "normalizer = (((Q1+Q2)>0).sum(axis=1)+1)*0+1\n",
    "row_indices, col_indices = test_X.nonzero()\n",
    "test_X.data /= normalizer[row_indices].A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cls = clsClass(verbose=True, max_iter=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/neural_network/multilayer_perceptron.py:565: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_probs = clf.predict_proba(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>test_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.026108</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.285429</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.408960</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.007848</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.325358</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_duplicate  test_id\n",
       "0      0.026108        0\n",
       "1      0.285429        1\n",
       "2      0.408960        2\n",
       "3      0.007848        3\n",
       "4      0.325358        4"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub = pd.DataFrame({'test_id': vectorized_test['test_id'], 'is_duplicate': test_probs[:,1]})\n",
    "sub.to_csv('multilayer_submission.csv', index=False)\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>test_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.026108</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.285429</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.408960</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.007848</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.325358</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_duplicate  test_id\n",
       "0      0.026108        0\n",
       "1      0.285429        1\n",
       "2      0.408960        2\n",
       "3      0.007848        3\n",
       "4      0.325358        4"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3eedc845377e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cdata/train_cleaned.pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquestion1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data = pd\n",
    "data.wob = data.question1\n",
    "same = [[w for w in x[3] if w in x[4]] for x in data.values]\n",
    "diff = [[w for w in x[3] if w not in x[4]] + [w for w in x[4] if w not in x[3]] for x in data.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
