{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import dill\n",
    "from IPython.core import display as ICD  # to print multiple nice pandas tables\n",
    "import json\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import tqdm\n",
    "from tqdm import tqdm_notebook, tnrange\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import classify\n",
    "from simple_model import SimpleModel\n",
    "from testing import test_simple_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "results = defaultdict(dict)\n",
    "dump = defaultdict(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_w(model, dataset, alpha=0.01, epochs=150):\n",
    "    train_scores = []\n",
    "    valid_scores = []\n",
    "    test_scores = []\n",
    "    model.fit(dataset.train_samples(), dataset.train_labels())\n",
    "    if epochs is None:\n",
    "        t = tnrange(100000)\n",
    "    else:\n",
    "        t = tnrange(epochs)\n",
    "    for e in t:\n",
    "        w = model.get_matrix_w()\n",
    "        w -= alpha * model.dw(dataset.train_samples(), dataset.train_labels())\n",
    "        model.save_matrix_w(w)\n",
    "        model.fit(dataset.train_samples(), dataset.train_labels())\n",
    "\n",
    "        train_score = model.score(dataset.train_samples(), dataset.train_labels())\n",
    "        valid_score = model.score(dataset.valid_samples(), dataset.valid_labels())\n",
    "        test_score = model.score(dataset.test_samples(), dataset.test_labels())\n",
    "        train_scores.append(train_score)\n",
    "        valid_scores.append(valid_score)\n",
    "        test_scores.append(test_score)\n",
    "        t.set_postfix(train_score=train_score, valid_score=valid_score, test_score=test_score)\n",
    "        \n",
    "        if epochs is None and e > 30:\n",
    "            end_mean = np.mean(valid_scores[-10:])\n",
    "            previos_mean = np.mean(valid_scores[-20:-10])\n",
    "            t.set_postfix(train_score=train_score, valid_score=valid_score, test_score=test_score, previos=previos_mean, end=end_mean)\n",
    "            if end_mean < previos_mean:\n",
    "                break\n",
    "    plt.plot(train_scores)\n",
    "    plt.plot(valid_scores)\n",
    "    plt.plot(test_scores)\n",
    "    plt.legend(['train', 'valid', 'test'])\n",
    "    plt.show()\n",
    "    return train_scores, valid_scores, test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_simple_model_with_gradient(\n",
    "    model, dataset, gradient_iters=300, dims=300, alpha=0.01, tag=None, results=None, dump=None, with_models=False, folds=1):\n",
    "    \n",
    "    for i in dataset.reshufle(None, folds):\n",
    "        model.internal_w=None\n",
    "        train_ps, valid_ps, test_ps = gradient_w(model, dataset, alpha, gradient_iters)\n",
    "        #train_p = model.score(dataset.train_samples(), dataset.train_labels())\n",
    "        #test_p = model.score(dataset.test_samples(), dataset.test_labels())\n",
    "\n",
    "        train_p = np.mean(train_ps[-10:])\n",
    "        valid_p = np.mean(valid_ps[-10:])\n",
    "        test_p = np.mean(test_ps[-10:])\n",
    "        if results is not None:\n",
    "            results[dataset.name()][('batch', tag, alpha, dims, 'train', i)] = train_p\n",
    "            results[dataset.name()][('batch', tag, alpha, dims, 'valid', i)] = valid_p\n",
    "            results[dataset.name()][('batch', tag, alpha, dims, 'test', i)] = test_p\n",
    "\n",
    "        if dump is not None:\n",
    "            dump[dataset.name()][('batch', tag, alpha, i)] = {\n",
    "                'train': list(train_ps),\n",
    "                'valid': list(valid_ps),\n",
    "                'test': list(test_ps),\n",
    "                'w': model.internal_w,\n",
    "            }\n",
    "            if with_models:\n",
    "                dump[dataset.name()][('batch', tag, alpha, i)]['model']= model\n",
    "\n",
    "\n",
    "        #raw_results[dataset.name()][('gradientw', tag, alpha)] = (train_ps, test_ps)\n",
    "        print(dataset.name())\n",
    "        print(\"Train precision\", train_p)\n",
    "        print(\"Valid precision\", valid_p)\n",
    "        print(\"Test precision\", test_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(args):\n",
    "    start = True\n",
    "    start_on = ' '\n",
    "    dump_results = True\n",
    "\n",
    "    result_file_pattern = 'dumps_new/batch_results_{}.pickle'\n",
    "    dump_file_pattern = 'dumps_new/batch_dump_{}.pickle'\n",
    "    results = defaultdict(dict)\n",
    "    dump = defaultdict(dict)\n",
    "\n",
    "    \n",
    "    dataset, scheme, alpha, dims = args\n",
    "    tag = '{}_{}_{}_{}'.format(dataset.name(), scheme, alpha, dims)\n",
    "    if not start:\n",
    "        start = (tag == start_on)\n",
    "        return\n",
    "    print(dataset.name(), scheme, alpha, dims, tag)\n",
    "    model = SimpleModel(classify.SkClassifier(), use_svd=True, weights=scheme, svd_dim=dims)\n",
    "    test_simple_model_with_gradient(\n",
    "        model, dataset, alpha=alpha, dims=dims, tag=scheme, \n",
    "        gradient_iters=None, results=results, dump=dump, with_models=False, folds=5)\n",
    "    print(list(model.internal_w.items())[:10])\n",
    "    results_file = result_file_pattern.format(tag)\n",
    "    dumps_file = dump_file_pattern.format(tag)\n",
    "    if dump_results:\n",
    "        pickle.dump(results, open(results_file, 'bw'))\n",
    "        pickle.dump(dump, open(dumps_file, 'bw'))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CRDataset None 0.1 200 CRDataset_None_0.1_200\n",
      "\n",
      "TRECDataset-HUM None 0.001 300 TRECDataset-HUM_None_0.001_300\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24dc0355dd274872b5c3730f30c75f37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=100000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4348025584bd408cb31098da6e30613e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=100000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-18:\n",
      "Process ForkPoolWorker-19:\n",
      "Traceback (most recent call last):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "Traceback (most recent call last):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-f695bb34ccf3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         '''\n\u001b[0;32m--> 260\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    547\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-32-9e1da91552f0>\", line 21, in run_test\n",
      "    gradient_iters=None, results=results, dump=dump, with_models=False, folds=5)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"<ipython-input-24-6afa83de07fa>\", line 6, in test_simple_model_with_gradient\n",
      "    train_ps, valid_ps, test_ps = gradient_w(model, dataset, alpha, gradient_iters)\n",
      "  File \"<ipython-input-32-9e1da91552f0>\", line 21, in run_test\n",
      "    gradient_iters=None, results=results, dump=dump, with_models=False, folds=5)\n",
      "  File \"<ipython-input-24-6afa83de07fa>\", line 6, in test_simple_model_with_gradient\n",
      "    train_ps, valid_ps, test_ps = gradient_w(model, dataset, alpha, gradient_iters)\n",
      "  File \"<ipython-input-23-f0c1f8d7b098>\", line 14, in gradient_w\n",
      "    model.fit(dataset.train_samples(), dataset.train_labels())\n",
      "  File \"/home/vlejd/Documents/school/diplomka/diploma_thesis/lsa_backprop/simple_model.py\", line 56, in fit\n",
      "    self.dictionary = gensim.corpora.Dictionary(X)\n",
      "  File \"<ipython-input-23-f0c1f8d7b098>\", line 14, in gradient_w\n",
      "    model.fit(dataset.train_samples(), dataset.train_labels())\n",
      "  File \"/home/vlejd/Documents/school/diplomka/diploma_thesis/lsa_backprop/simple_model.py\", line 69, in fit\n",
      "    self.corpus = gensim.matutils.corpus2dense(self.lsi[bow], self.lsi.num_topics).T\n",
      "  File \"/home/vlejd/.virtualenvs/diplomka/lib/python3.5/site-packages/gensim/corpora/dictionary.py\", line 58, in __init__\n",
      "    self.add_documents(documents, prune_at=prune_at)\n",
      "  File \"/home/vlejd/.virtualenvs/diplomka/lib/python3.5/site-packages/gensim/corpora/dictionary.py\", line 119, in add_documents\n",
      "    self.doc2bow(document, allow_update=True)  # ignore the result, here we only care about updating token ids\n",
      "  File \"/home/vlejd/.virtualenvs/diplomka/lib/python3.5/site-packages/gensim/matutils.py\", line 300, in corpus2dense\n",
      "    result = np.column_stack(sparse2full(doc, num_terms) for doc in corpus)\n",
      "  File \"/home/vlejd/.virtualenvs/diplomka/lib/python3.5/site-packages/numpy/lib/shape_base.py\", line 364, in column_stack\n",
      "    for v in tup:\n",
      "  File \"/home/vlejd/.virtualenvs/diplomka/lib/python3.5/site-packages/gensim/matutils.py\", line 300, in <genexpr>\n",
      "    result = np.column_stack(sparse2full(doc, num_terms) for doc in corpus)\n",
      "  File \"/home/vlejd/.virtualenvs/diplomka/lib/python3.5/site-packages/gensim/corpora/dictionary.py\", line 158, in doc2bow\n",
      "    result = {token2id[w]: freq for w, freq in iteritems(counter) if w in token2id}\n",
      "  File \"/home/vlejd/.virtualenvs/diplomka/lib/python3.5/site-packages/gensim/corpora/dictionary.py\", line 158, in <dictcomp>\n",
      "    result = {token2id[w]: freq for w, freq in iteritems(counter) if w in token2id}\n",
      "  File \"/home/vlejd/.virtualenvs/diplomka/lib/python3.5/site-packages/gensim/interfaces.py\", line 120, in __iter__\n",
      "    for chunk in utils.grouper(self.corpus, self.chunksize):\n",
      "  File \"/home/vlejd/.virtualenvs/diplomka/lib/python3.5/site-packages/gensim/utils.py\", line 816, in chunkize_serial\n",
      "    wrapped_chunk = [list(itertools.islice(it, int(chunksize)))]\n",
      "  File \"/home/vlejd/.virtualenvs/diplomka/lib/python3.5/site-packages/gensim/matutils.py\", line 345, in __iter__\n",
      "    yield list(zip(self.sparse.indices[indprev:indnow], self.sparse.data[indprev:indnow]))\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "args = []\n",
    "\n",
    "for scheme in SimpleModel.SCHEMES[::2]:\n",
    "    for alpha in [0.1, 0.01, 0.001]:\n",
    "        for dims in [200, 300, 400]:\n",
    "            for dataset in datasets.ALL_DATASETS+ datasets.TREC_DATASETS:\n",
    "                args.append((dataset, scheme, alpha, dims))\n",
    "\n",
    "with Pool(2) as p:\n",
    "    print(p.map(run_test, args))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diplomka",
   "language": "python",
   "name": "diplomka"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
